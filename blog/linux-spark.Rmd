---
title: "Linux Spark 掉坑爬坑记"
output: html_notebook
---

# 1 安装软件
Linux下安装软件于windows有很大不同。大致来讲，有4种方式，有的地方会更加的方便，但有的地方也非常的不方便。

## 1.1 linux下安装软件的四种方法。

* 第1种记在线用命令安装，非常方便，敲一行命令，就可以直接安装一个软件，网上的攻略也是经常是用这种方式，`pip install SomePackage`


* 第2种是下载预编译包，这大致相当于windows下的绿色软件，下载的软件包直接解压，再修改配置文件，设置环境变量，就好了。

* 第3种是sh文件，这是类似与windows下如exe安装包，只要下载好，用命令安执行，按照提示一步一步操作就可以了，只是把用鼠标点击一步步安装，改成了敲击键盘和回车。

* 第四种是最复杂如，下载软件的源代码，解压后自己编译，不但要使用编译命令，还必须要有编译的环境，一般是gcc。

* 我采用的方法。
第一种方法虽然好，但我们是无法使用的，因为我们的服务器与外网是隔离的。我用的最多的是第二种方法，首先用自己的电脑，首先把压缩包下载好，查杀病毒，上传到服务器上，解压配置文件。虽然看上去还算简单，但Linux下压缩包格式有很多种，有tar、gz等等，不同的文件要用不同的解压命令，几乎是没有办法记住，只能现用现查。

## 1.2 安装步骤
### 1.2.1 安装java，第一个坑
前面的步骤一样，我下载如记rpm包，安装本省挺顺利。
修改配置文件是很大的坑，千万不能出错，用这个命令打开配置文件，`vi /etc/profile`

修改这个文件，配置环境变量，java需要配置好几个环境变量，PATH，CLASSPATH，JAVA_HOME，
这个配置文件，环境变量命令如下：

`export JAVA_HOME=/usr/share/jdk1.6.0_14'`

`export PATH=\$JAVA_HOME/bin:$PATH`

每个变量用`：`隔开，而windows是用`；`, 连接用`$PATH`，千万不能出错

我第一次就把冒号输入成了分号，导致把整个PATH都没有用了，几乎所有命令都用不了
vi命令也用不了，所以也没有办法打开配置文件修改，真是难题，这记第一根坑

* 解决，PATH没有了，直接打命令找不到，但命令本身还是在的，只要的文件夹种找到，还是可以用如，
在各个文件夹中找到了`vi`，切换到文件夹中后，重新修改。但是修改后的`source`命令到处都没有找到，
好在只要重启，就可以自动执行，继续找`shutdown`命令，然后用`shutdown restart now`。
重启后终于好了。

### 1.2.2 安装scala
![](scala-spiral.png ){width=4%}也用rpm包，一切顺利


### 1.2.3 安装Hadoop
![](hadoop-logo.jpg){width=20%}tar.gz包，下载了2.7的，一切顺利，但暂时用不到，我就没有去配置。

### 1.2.4 安装Spark
![](spark-logo-trademark.png){width=10%}终于到正题了，下载了spark-2.4.0-bin-hadoop2.7.tgz，貌似自带hadoop，不过我也没时间试过了。


# 2 应用篇
## 2.1 端口篇
粗粗查了查，spark要用到8080，8081，7077，4040，联系数管开放端口。
又掉小坑，错看成7070，不过没关系，改一下配置应该就可以解决。

* 解决

只要输入命令时指定端口就OK了，不用改配置文件也可以 
`SPARK_MASTER_PORT=7070 sbin/start-all.sh`
然后在自己办公电脑上输入spark://ip:8080，
显示spark在7070端口运行


## 2.2 试用spark-shell
安装完成后，敲击命令`spark-shell`就可以出现这样的界面，说明没问题了。
因为spark是用scala写的，所以spark的shell是scala语言的。

##  2.3  试用pyspark
我主要还是想用python写spark的代码，所以要用pyspark，这个装好spark也都是自带的。
但敲入命令`pyspark`后提示错误，。

##  2.4  大坑——版本问题
经过反复测试和搜寻，包括下载安装，终于确定这个错误的原因，应该的是python的版本问题。
我的系统是CentOS6.5，默认的Python是2.6，似乎Pyspark并不支持2.6。
所以接下来，我要几条路可走，1升级Python到2.7；2用Anaconda直接升级到3.6；3重新环系统。

* 解决

第3条路肯定是放在最后了，考虑到pyspark用2.7的人多，优先选择这第一条路，
但实际上很难，Python2.7就属于前面讲到安装软件的第四种，需要下载源代码，自行编译的，
我测试了下，CentOS6.5没有默认安装编译器，而且搜索了下，无网安装编译器很复杂。
所以不得不考虑第2条路，虽然有奉献，Python3的特性有的可能Pyspark不支持，但目前先这样试试吧。
下载Anaconda，这个就是软件安装的第3种——sh文件，执行后安提示安装，顺利。
然后再次修改配置，配置环境变量，并把新的Python环境设为系统默认。
听说升级为Python3后，系统的Yam就用不了了，不过目前还没发现异常。
重启，再次敲`pyspark`顺利出现Spark界面，终于搞定可以开工了。

## 2.5  Jupyter
能用之后，就得考虑好用，既然安装了Jupyter，当然想用jupyter notebook作为开发环境，
这样我也不用频繁登录服务器了，可以直接在自己电脑上登录服务器的jupyter notebook写和测试代码了。
Anaconda3都是自带jupyter的，找到配置文件，修改密码，端口还是默认的8888，修改为root可用，
`c.NotebookApp.allow_root = True`。


之后需要设置密码，方法是先用Python生成密匙的SHA1散列码，
```{python}
from notebook.auth import passwd
passwd()
```

输入密匙后将SHA1码复制到设置文件中，
`c.NotebookApp.password = u'sha1:'`。

`jupyter notebook`，提示已部署，但在自己电脑上就是上不了。

* 解决

不断与数管中心确认端口后，也用`iptables `查看了，端口确实开放，
再次搜索，发现Jupyter的配置文件中还有个ip控制的设置，
将允许访问ip改为星号，`c.NotebookApp.ip = '*'`，然后继续报错，
还有个也要修改，`c.NotebookApp.allow_remote_access = True`，
再次启动，在自己电脑上访问，成功！

## 还有后续
下篇将介绍学写spark代码中的收获
