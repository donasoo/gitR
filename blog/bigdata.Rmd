---
title: "我就是这么掉进大数据坑的"
output: html_notebook
---

# 1. 我就不能闲着吗？为什么总想着解决问题
## 1.1 事情由来——农普数据处理的难题
用R做数据处理是很高的，可以灵活对数据进行拼接、变型等操作，
再做汇总就方便很多，但R也有个缺点，必须将数据读入内存，
我们的办公电脑只有4G内存，在系统用掉一些，就没有多少留给R了，
**10列左右的数据，百万行量级是极限了**，
而农普H表本身已有800多万行，而且列数只有上百，
因此无法在R中处理，更何况拼接分组后可能在此基础上再扩大100倍。

##1.2 调查过程中的方法
在农普调查审核阶段，尚有折中方法克服上述难题，
从数据库读取数据时**先用sql进行汇总，汇总到村级**，
这样能把数据行数缩小到万行量级，而且一般仅选择当时需要的指标，
能保证即使做数据变型，数据行数基本能控制在千万行以内。

## 1.3 农普资料开发的新难题
上述处理方法在农普资料开发中也不管用了，
首先农普资料开发囊括了大部指标，没有了选择指标缩小列数的可能，
其次，资料中按村分组可能通过sql先行汇总的方式缩小数据行数，
其他按户、人属性分组的没有办法预汇总且显著缩小数据行数。

##  1.4 新处理方法对农普资料开发的必要性
普查资料开发是一种规律性很强的数据处理方式，
可以总结为按少数几种预订的分组汇总所有指标，
汇总的方式也只有求和和计算，汇总后再计算百分比。
而现有的数据处理系统，无论是农普国家平台，还是现省平台，
在数据汇总方面都是强调个性化，以尽可能的满足多样性的汇总要求，
代价就是牺牲了效率，无论制表还是汇总数据都很慢。
而这种规则性很强的普查资料开发，正是R、Python等语言的强项，
但必须得解决大数据难题。

#  2. 新契机
##  2.1 农村司的解决方案
上饶国家资料开发会中，农村司拿出了自己的解决方案，
**MicroSoft Machine Learning Server**，
万国强处长看来是有个大计划，1年前在咸宁时候培训了R，原来是有的放矢。
MMLS是微软的大数据处理平台，R和Python都可用，可部署到集群，
节点为三种类型，主、计算和web，4台组网是起步，当然单季运行也没问题，
我们培训时，笔记本电脑都能运行单机平台。

##  2.2 另辟蹊径
传统中，微软并不注重大数据，当Hadoop，BigTable横空出世时，微软还不直到在干嘛，
既然如此，我为何不直接从更加大众的大数据平台入手呢，于是Spark就这么进入了我的视野。
在Spark的论文中，性能是Hadoop的100倍，虽然只进行了大家都能想到的优化，
用RDD代替了HDFS（这么说也许并不准确），在数据处理的过程中不用每次的读写HDFS，
而是更多在内存中处理，如此就快了100倍，**Spark如日中天，我就这么选了Spark**。

## 2.3 准备工作
和处理汇报后，并与数管中心褚英国主任商量后，征得了一个服务器1年的使用权，
OA提交申请，服务器到位，虽然只有一台，但可以开工了，欧耶！！